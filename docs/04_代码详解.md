# ğŸ’» ä»£ç è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Šé¡¹ç›®ä¸­æ¯ä¸ªæ–‡ä»¶çš„ä½œç”¨å’Œæ ¸å¿ƒä»£ç å®ç°ï¼Œå¸®åŠ©åˆå­¦è€…ç†è§£å’Œä¿®æ”¹ä»£ç ã€‚

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ–‡ä»¶æ¦‚è§ˆ](#é¡¹ç›®æ–‡ä»¶æ¦‚è§ˆ)
2. [model.py - U-Net æ¨¡å‹](#modelpy---u-net-æ¨¡å‹)
3. [dataset.py - æ•°æ®åŠ è½½](#datasetpy---æ•°æ®åŠ è½½)
4. [model_train.py - æ¨¡å‹è®­ç»ƒ](#model_trainpy---æ¨¡å‹è®­ç»ƒ)
5. [app.py - Web åº”ç”¨](#apppy---web-åº”ç”¨)
6. [å¦‚ä½•ä¿®æ”¹å’Œæ‰©å±•](#å¦‚ä½•ä¿®æ”¹å’Œæ‰©å±•)

---

## é¡¹ç›®æ–‡ä»¶æ¦‚è§ˆ

### æ ¸å¿ƒä»£ç æ–‡ä»¶

| æ–‡ä»¶ | è¡Œæ•° | ä½œç”¨ | éš¾åº¦ |
|------|-----|------|------|
| **model.py** | ~76 è¡Œ | å®šä¹‰ U-Net ç¥ç»ç½‘ç»œç»“æ„ | â­â­â­ |
| **dataset.py** | ~152 è¡Œ | åŠ è½½å’Œé¢„å¤„ç†æ•°æ® | â­â­ |
| **model_train.py** | ~302 è¡Œ | è®­ç»ƒæ¨¡å‹çš„ä¸»ç¨‹åº | â­â­â­â­ |
| **model_test.py** | ~160 è¡Œ | æµ‹è¯•å’Œè¯„ä¼°æ¨¡å‹ | â­â­ |
| **app.py** | ~221 è¡Œ | Web åº”ç”¨ç¨‹åº | â­â­â­ |

### è¾…åŠ©æ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ |
|------|------|
| **requirements.txt** | Python ä¾èµ–åŒ…åˆ—è¡¨ |
| **run_linux.sh** | Linux å¯åŠ¨è„šæœ¬ |
| **run_windows.bat** | Windows å¯åŠ¨è„šæœ¬ |
| **create_val_set.py** | æ•°æ®é›†åˆ’åˆ†è„šæœ¬ |
| **mean_std.py** | è®¡ç®—æ•°æ®é›†å‡å€¼å’Œæ ‡å‡†å·® |

---

## model.py - U-Net æ¨¡å‹

### æ–‡ä»¶ä½œç”¨

å®šä¹‰ U-Net ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚U-Net æ˜¯ä¸€ç§ç”¨äºå›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

### æ ¸å¿ƒç»„ä»¶

#### 1. DoubleConv æ¨¡å—

**ä½œç”¨**ï¼šU-Net çš„åŸºæœ¬æ„å»ºå—ï¼Œè¿›è¡Œä¸¤æ¬¡å·ç§¯æ“ä½œ

```python
class DoubleConv(nn.Module):
    """(å·ç§¯ => BN => ReLU) * 2"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.double_conv = nn.Sequential(
            # ç¬¬ä¸€æ¬¡å·ç§¯
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),  # æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),          # æ¿€æ´»å‡½æ•°

            # ç¬¬äºŒæ¬¡å·ç§¯
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)
```

**å‚æ•°è§£é‡Š**ï¼š
- `in_channels`: è¾“å…¥é€šé“æ•°ï¼ˆå¦‚ RGB å›¾ç‰‡ = 3ï¼‰
- `out_channels`: è¾“å‡ºé€šé“æ•°ï¼ˆç‰¹å¾å›¾æ•°é‡ï¼‰
- `kernel_size=3`: ä½¿ç”¨ 3Ã—3 çš„å·ç§¯æ ¸
- `padding=1`: å¡«å……ï¼Œä¿æŒå›¾ç‰‡å°ºå¯¸ä¸å˜

**é€šä¿—ç†è§£**ï¼š
æƒ³è±¡ä½ æœ‰ä¸€å¼ å›¾ç‰‡ï¼ŒDoubleConv åƒæ˜¯ç»™å›¾ç‰‡åŠ ä¸¤å±‚"æ»¤é•œ"ï¼Œæ¯å±‚æ»¤é•œæå–ä¸åŒçš„ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ç­‰ï¼‰ã€‚

#### 2. UNet ä¸»ä½“ç»“æ„

**æ•´ä½“æ¶æ„**ï¼šU å‹ç»“æ„ï¼ˆå…ˆå‹ç¼©å†æ‰©å¼ ï¼‰

```python
class UNet(nn.Module):
    def __init__(self, n_channels=3, n_classes=2):
        super(UNet, self).__init__()

        # ===== ç¼–ç å™¨ï¼ˆä¸‹é‡‡æ ·è·¯å¾„ï¼‰=====
        # é€æ­¥æå–ç‰¹å¾ï¼ŒåŒæ—¶ç¼©å°å›¾ç‰‡å°ºå¯¸

        self.inc = DoubleConv(n_channels, 64)    # 3 â†’ 64 é€šé“
        self.down1 = nn.Sequential(
            nn.MaxPool2d(2),                      # å°ºå¯¸å‡åŠ
            DoubleConv(64, 128)                   # 64 â†’ 128 é€šé“
        )
        self.down2 = nn.Sequential(
            nn.MaxPool2d(2),                      # å†å‡åŠ
            DoubleConv(128, 256)                  # 128 â†’ 256 é€šé“
        )
        self.down3 = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(256, 512)
        )
        self.down4 = nn.Sequential(
            nn.MaxPool2d(2),
            DoubleConv(512, 1024)                 # æœ€æ·±å±‚ï¼š1024 é€šé“
        )

        # ===== è§£ç å™¨ï¼ˆä¸Šé‡‡æ ·è·¯å¾„ï¼‰=====
        # é€æ­¥æ¢å¤å›¾ç‰‡å°ºå¯¸ï¼ŒåŒæ—¶èåˆé«˜ä½å±‚ç‰¹å¾

        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.conv1 = DoubleConv(1024, 512)  # 1024 = 512(ä¸Šé‡‡æ ·) + 512(è·³è·ƒè¿æ¥)

        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.conv2 = DoubleConv(512, 256)

        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.conv3 = DoubleConv(256, 128)

        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.conv4 = DoubleConv(128, 64)

        # ===== è¾“å‡ºå±‚ =====
        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)  # 64 â†’ 2 ç±»åˆ«
```

**å°ºå¯¸å˜åŒ–ç¤ºä¾‹**ï¼ˆè¾“å…¥ 224Ã—224Ã—3ï¼‰ï¼š

```
è¾“å…¥: [224Ã—224Ã—3]
  â†“ inc
[224Ã—224Ã—64]
  â†“ down1 (MaxPool2d)
[112Ã—112Ã—128]
  â†“ down2
[56Ã—56Ã—256]
  â†“ down3
[28Ã—28Ã—512]
  â†“ down4
[14Ã—14Ã—1024]  â† æœ€æ·±å±‚ï¼ˆç“¶é¢ˆï¼‰
  â†“ up1
[28Ã—28Ã—512]   â† ä¸Šé‡‡æ · + è·³è·ƒè¿æ¥
  â†“ up2
[56Ã—56Ã—256]
  â†“ up3
[112Ã—112Ã—128]
  â†“ up4
[224Ã—224Ã—64]
  â†“ outc
è¾“å‡º: [224Ã—224Ã—2]  (2ä¸ªç±»åˆ«ï¼šèƒŒæ™¯ã€è£‚ç¼)
```

#### 3. å‰å‘ä¼ æ’­

**è·³è·ƒè¿æ¥**ï¼šç¼–ç å™¨çš„ç‰¹å¾ç›´æ¥ä¼ é€’ç»™è§£ç å™¨

```python
def forward(self, x):
    # ç¼–ç è·¯å¾„ - ä¿å­˜ä¸­é—´ç‰¹å¾
    x1 = self.inc(x)         # [224Ã—224Ã—64]
    x2 = self.down1(x1)      # [112Ã—112Ã—128]
    x3 = self.down2(x2)      # [56Ã—56Ã—256]
    x4 = self.down3(x3)      # [28Ã—28Ã—512]
    x5 = self.down4(x4)      # [14Ã—14Ã—1024]

    # è§£ç è·¯å¾„ - èåˆç‰¹å¾
    x = self.up1(x5)                    # ä¸Šé‡‡æ ·: [28Ã—28Ã—512]
    x = torch.cat([x, x4], dim=1)       # æ‹¼æ¥: [28Ã—28Ã—1024]
    x = self.conv1(x)                   # å·ç§¯: [28Ã—28Ã—512]

    x = self.up2(x)                     # [56Ã—56Ã—256]
    x = torch.cat([x, x3], dim=1)       # [56Ã—56Ã—512]
    x = self.conv2(x)                   # [56Ã—56Ã—256]

    x = self.up3(x)                     # [112Ã—112Ã—128]
    x = torch.cat([x, x2], dim=1)       # [112Ã—112Ã—256]
    x = self.conv3(x)                   # [112Ã—112Ã—128]

    x = self.up4(x)                     # [224Ã—224Ã—64]
    x = torch.cat([x, x1], dim=1)       # [224Ã—224Ã—128]
    x = self.conv4(x)                   # [224Ã—224Ã—64]

    # è¾“å‡ºåˆ†å‰²ç»“æœ
    logits = self.outc(x)               # [224Ã—224Ã—2]
    return logits
```

**ä¸ºä»€ä¹ˆéœ€è¦è·³è·ƒè¿æ¥ï¼Ÿ**
- ç¼–ç å™¨ï¼šæå–é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼ˆ"è¿™æ˜¯è£‚ç¼"ï¼‰
- è·³è·ƒè¿æ¥ï¼šä¿ç•™ä½å±‚ç»†èŠ‚ç‰¹å¾ï¼ˆ"è£‚ç¼çš„ç²¾ç¡®ä½ç½®"ï¼‰
- ç»“åˆä¸¤è€…ï¼šå‡†ç¡®å®šä½è£‚ç¼è¾¹ç•Œ

---

## dataset.py - æ•°æ®åŠ è½½

### æ–‡ä»¶ä½œç”¨

è´Ÿè´£è¯»å–å›¾ç‰‡å’Œæ ‡æ³¨ï¼Œè½¬æ¢ä¸º PyTorch å¯ä»¥å¤„ç†çš„å¼ é‡æ ¼å¼ã€‚

### æ ¸å¿ƒç±»ï¼šCrackDataset

```python
class CrackDataset(Dataset):
    """é“è·¯è£‚ç¼åˆ†å‰²æ•°æ®é›†"""

    def __init__(self, image_dir, mask_dir, transform=None, img_size=(224, 224)):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            image_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            mask_dir: æ©è†œæ–‡ä»¶å¤¹è·¯å¾„
            transform: æ•°æ®å¢å¼ºå˜æ¢
            img_size: å›¾ç‰‡å°ºå¯¸ (height, width)
        """
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.img_size = img_size

        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶å
        self.images = sorted([
            f for f in os.listdir(image_dir)
            if f.endswith(('.jpg', '.png', '.jpeg', '.JPG', '.PNG'))
        ])

        if len(self.images) == 0:
            raise ValueError(f"âŒ åœ¨ {image_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼")

        print(f"âœ“ åŠ è½½æ•°æ®é›†: {len(self.images)} å¼ å›¾ç‰‡")
```

**é€šä¿—ç†è§£**ï¼š
`CrackDataset` åƒä¸€ä¸ª"å›¾ä¹¦ç®¡ç†å‘˜"ï¼Œå®ƒçŸ¥é“æ‰€æœ‰å›¾ç‰‡å’Œæ ‡æ³¨çš„ä½ç½®ï¼Œå½“ä½ éœ€è¦æŸå¼ å›¾ç‰‡æ—¶ï¼Œå®ƒä¼šå¸®ä½ æ‰¾åˆ°å¹¶å‡†å¤‡å¥½ã€‚

### æ•°æ®è¯»å–

```python
def __getitem__(self, idx):
    """è·å–ç¬¬ idx ä¸ªæ ·æœ¬"""

    # 1. è¯»å–å›¾ç‰‡
    img_name = self.images[idx]
    img_path = os.path.join(self.image_dir, img_name)

    # 2. è¯»å–å¯¹åº”çš„æ ‡æ³¨ï¼ˆå°è¯•å¤šç§æ‰©å±•åï¼‰
    mask_name = os.path.splitext(img_name)[0]  # å»æ‰æ‰©å±•å
    mask_path = None
    for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG']:
        temp_path = os.path.join(self.mask_dir, mask_name + ext)
        if os.path.exists(temp_path):
            mask_path = temp_path
            break

    if mask_path is None:
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ©è†œæ–‡ä»¶: {mask_name}")

    # 3. è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
    image = cv2.imread(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB
    image = cv2.resize(image, self.img_size)        # è°ƒæ•´å°ºå¯¸

    # 4. è¯»å–å’Œé¢„å¤„ç†æ©è†œ
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    mask = cv2.resize(mask, self.img_size)
    mask = (mask > 127).astype(np.float32)  # äºŒå€¼åŒ–ï¼š>127è®¾ä¸º1ï¼Œå¦åˆ™ä¸º0

    # 5. åº”ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚æœæœ‰ï¼‰
    if self.transform:
        image = Image.fromarray(image)
        image = self.transform(image)  # å½’ä¸€åŒ–ç­‰æ“ä½œ
    else:
        # è½¬æ¢ä¸º tensor
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0

    # 6. æ©è†œè½¬ tensor
    mask = torch.from_numpy(mask).unsqueeze(0).float()

    return image, mask  # è¿”å›å›¾ç‰‡å’Œæ ‡æ³¨
```

**æ•°æ®æµç¨‹**ï¼š
```
åŸå§‹å›¾ç‰‡ (1920Ã—1080Ã—3)
  â†’ è°ƒæ•´å°ºå¯¸ (224Ã—224Ã—3)
  â†’ å½’ä¸€åŒ– ([0,1])
  â†’ è½¬æ¢ä¸ºtensor (3Ã—224Ã—224)

åŸå§‹æ ‡æ³¨ (1920Ã—1080)
  â†’ è°ƒæ•´å°ºå¯¸ (224Ã—224)
  â†’ äºŒå€¼åŒ– (0æˆ–1)
  â†’ è½¬æ¢ä¸ºtensor (1Ã—224Ã—224)
```

---

## model_train.py - æ¨¡å‹è®­ç»ƒ

### æ–‡ä»¶ä½œç”¨

è®­ç»ƒ U-Net æ¨¡å‹çš„ä¸»ç¨‹åºï¼ŒåŒ…å«æ•°æ®åŠ è½½ã€è®­ç»ƒå¾ªç¯ã€éªŒè¯ç­‰ã€‚

### 1. æ•°æ®åŠ è½½å‡½æ•°

```python
def train_val_data_process():
    """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""

    # æ•°æ®è·¯å¾„
    train_image_path = 'data/train/images'
    train_mask_path = 'data/train/masks'
    val_image_path = 'data/val/images'
    val_mask_path = 'data/val/masks'

    # å½’ä¸€åŒ–å‚æ•°ï¼ˆé¢„å…ˆè®¡ç®—çš„æ•°æ®é›†å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    normalize = transforms.Normalize(
        mean=[0.15804266, 0.16457426, 0.16825973],  # R, G, B å‡å€¼
        std=[0.04205786, 0.04393576, 0.04547899]    # R, G, B æ ‡å‡†å·®
    )

    # æ•°æ®é¢„å¤„ç†æµç¨‹
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),  # è°ƒæ•´å°ºå¯¸
        transforms.ToTensor(),          # è½¬æ¢ä¸ºtensor
        normalize                       # æ ‡å‡†åŒ–
    ])

    # åˆ›å»ºæ•°æ®é›†
    train_data = RoadCrackDataset(train_image_path, train_mask_path,
                                  transform=train_transform)
    val_data = RoadCrackDataset(val_image_path, val_mask_path,
                                transform=train_transform)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 91
    train_loader = DataLoader(train_data, batch_size=batch_size,
                             shuffle=True, num_workers=12, pin_memory=True)
    val_loader = DataLoader(val_data, batch_size=batch_size,
                           shuffle=False, num_workers=12, pin_memory=True)

    return train_loader, val_loader
```

**å‚æ•°è¯´æ˜**ï¼š
- `batch_size`: æ¯æ¬¡è®­ç»ƒä½¿ç”¨çš„å›¾ç‰‡æ•°é‡
- `shuffle=True`: æ‰“ä¹±è®­ç»ƒæ•°æ®ï¼ˆéªŒè¯é›†ä¸æ‰“ä¹±ï¼‰
- `num_workers`: ä½¿ç”¨å¤šå°‘ä¸ªCPUæ ¸å¿ƒåŠ è½½æ•°æ®
- `pin_memory`: åŠ é€ŸGPUæ•°æ®ä¼ è¾“

### 2. IoU è®¡ç®—

```python
def calculate_iou(pred, target, num_classes=2):
    """
    è®¡ç®— IoU (Intersection over Union)

    Args:
        pred: é¢„æµ‹ç»“æœ [B, H, W]
        target: çœŸå®æ ‡æ³¨ [B, H, W]
        num_classes: ç±»åˆ«æ•°ï¼ˆèƒŒæ™¯+è£‚ç¼=2ï¼‰

    Returns:
        å¹³å‡ IoU
    """
    ious = []
    pred = pred.view(-1)      # å±•å¹³ä¸ºä¸€ç»´
    target = target.view(-1)

    for cls in range(num_classes):
        # æ‰¾åˆ°é¢„æµ‹ä¸ºè¯¥ç±»çš„åƒç´ 
        pred_inds = pred == cls
        # æ‰¾åˆ°çœŸå®ä¸ºè¯¥ç±»çš„åƒç´ 
        target_inds = target == cls

        # è®¡ç®—äº¤é›†å’Œå¹¶é›†
        intersection = (pred_inds & target_inds).sum().float()
        union = (pred_inds | target_inds).sum().float()

        if union == 0:
            ious.append(float('nan'))  # è¯¥ç±»åˆ«ä¸å­˜åœ¨
        else:
            ious.append((intersection / union).item())

    return np.nanmean(ious)  # è¿”å›å¹³å‡å€¼
```

**IoU å¯è§†åŒ–**ï¼š
```
çœŸå®æ ‡æ³¨:     é¢„æµ‹ç»“æœ:     äº¤é›†:         å¹¶é›†:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆ          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

IoU = äº¤é›†é¢ç§¯ / å¹¶é›†é¢ç§¯ = 13 / 15 = 0.867
```

### 3. è®­ç»ƒä¸»å‡½æ•°

```python
def train_model_process(model, train_dataloader, val_dataloader, num_epochs):
    """è®­ç»ƒæ¨¡å‹"""

    # 1. è®¾å¤‡é…ç½®
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨å¹¶è¡Œè®­ç»ƒ
    if torch.cuda.device_count() > 1:
        model = nn.DataParallel(model)

    # 2. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±

    # 3. è®°å½•æœ€ä½³æ¨¡å‹
    best_model_wts = copy.deepcopy(model.state_dict())
    best_iou = 0.0

    # 4. è®°å½•è®­ç»ƒè¿‡ç¨‹
    train_loss_all = []
    val_loss_all = []
    train_iou_all = []
    val_iou_all = []

    # 5. è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"Epoch {epoch}/{num_epochs-1}")

        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        train_loss = 0.0
        train_iou = 0.0
        train_num = 0

        for images, masks in train_dataloader:
            images = images.to(device)
            masks = masks.to(device, dtype=torch.long)

            # å‰å‘ä¼ æ’­
            output = model(images)           # [B, 2, H, W]
            loss = criterion(output, masks)  # è®¡ç®—æŸå¤±

            # åå‘ä¼ æ’­
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            loss.backward()        # è®¡ç®—æ¢¯åº¦
            optimizer.step()       # æ›´æ–°å‚æ•°

            # è®¡ç®—é¢„æµ‹ç»“æœ
            pred_mask = torch.argmax(output, dim=1)  # [B, H, W]

            # ç´¯åŠ æŒ‡æ ‡
            train_loss += loss.item() * images.size(0)
            train_iou += calculate_iou(pred_mask, masks) * images.size(0)
            train_num += images.size(0)

        # ===== éªŒè¯é˜¶æ®µ =====
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        val_loss = 0.0
        val_iou = 0.0
        val_num = 0

        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
            for images, masks in val_dataloader:
                images = images.to(device)
                masks = masks.to(device, dtype=torch.long)

                output = model(images)
                loss = criterion(output, masks)
                pred_mask = torch.argmax(output, dim=1)

                val_loss += loss.item() * images.size(0)
                val_iou += calculate_iou(pred_mask, masks) * images.size(0)
                val_num += images.size(0)

        # 6. è®¡ç®—å¹³å‡æŒ‡æ ‡
        train_loss_all.append(train_loss / train_num)
        train_iou_all.append(train_iou / train_num)
        val_loss_all.append(val_loss / val_num)
        val_iou_all.append(val_iou / val_num)

        print(f'Train Loss: {train_loss_all[-1]:.4f} Train IoU: {train_iou_all[-1]:.4f}')
        print(f'Val Loss: {val_loss_all[-1]:.4f} Val IoU: {val_iou_all[-1]:.4f}')

        # 7. ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_iou_all[-1] > best_iou:
            best_iou = val_iou_all[-1]
            best_model_wts = copy.deepcopy(model.state_dict())

    # 8. åŠ è½½æœ€ä½³æ¨¡å‹å¹¶ä¿å­˜
    model.load_state_dict(best_model_wts)
    torch.save(model.state_dict(), 'best_model.pth')

    return train_process  # è¿”å›è®­ç»ƒå†å²
```

**è®­ç»ƒæµç¨‹å›¾**ï¼š
```
for each epoch:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   è®­ç»ƒé˜¶æ®µ          â”‚
  â”‚  1. å‰å‘ä¼ æ’­        â”‚
  â”‚  2. è®¡ç®—æŸå¤±        â”‚
  â”‚  3. åå‘ä¼ æ’­        â”‚
  â”‚  4. æ›´æ–°å‚æ•°        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   éªŒè¯é˜¶æ®µ          â”‚
  â”‚  1. å‰å‘ä¼ æ’­        â”‚
  â”‚  2. è®¡ç®—æŒ‡æ ‡        â”‚
  â”‚  3. ä¸æ›´æ–°å‚æ•°      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  è®°å½•å’Œä¿å­˜         â”‚
  â”‚  1. è®°å½•Loss/IoU    â”‚
  â”‚  2. ä¿å­˜æœ€ä½³æ¨¡å‹    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## app.py - Web åº”ç”¨

### æ–‡ä»¶ä½œç”¨

ä½¿ç”¨ Gradio åˆ›å»º Web ç•Œé¢ï¼Œè®©ç”¨æˆ·å¯ä»¥é€šè¿‡æµè§ˆå™¨ä½¿ç”¨æ¨¡å‹ã€‚

### æ ¸å¿ƒç±»ï¼šCrackDetectionApp

```python
class CrackDetectionApp:
    def __init__(self, model_path='best_model.pth'):
        """åˆå§‹åŒ–åº”ç”¨"""

        # 1. è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # 2. åŠ è½½æ¨¡å‹
        self.model = UNet(n_channels=3, n_classes=2)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model = self.model.to(self.device)
        self.model.eval()  # è¯„ä¼°æ¨¡å¼

        # 3. æ•°æ®é¢„å¤„ç†
        self.normalize = transforms.Normalize(
            mean=[0.15804266, 0.16457426, 0.16825973],
            std=[0.04205786, 0.04393576, 0.04547899]
        )

        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            self.normalize
        ])
```

### å›¾ç‰‡å¤„ç†æµç¨‹

```python
def process_image(self, input_image, show_overlay=True, overlay_alpha=0.5):
    """å¤„ç†ä¸Šä¼ çš„å›¾åƒ"""

    # 1. é¢„å¤„ç†å›¾ç‰‡
    image_tensor, original_size = self.preprocess_image(input_image)

    # 2. æ¨¡å‹é¢„æµ‹
    pred_mask = self.predict(image_tensor)

    # 3. è®¡ç®—è£‚ç¼å æ¯”
    crack_percentage = self.calculate_crack_percentage(pred_mask)

    # 4. ç”Ÿæˆå¯è§†åŒ–
    mask_vis = (pred_mask * 255).astype(np.uint8)  # 0-1 â†’ 0-255
    overlay_image = self.create_overlay(input_image, pred_mask, overlay_alpha)

    # 5. ç”ŸæˆæŠ¥å‘Š
    if crack_percentage > 0.1:
        severity = "æ£€æµ‹åˆ°è£‚ç¼"
        if crack_percentage > 10:
            severity += " - ä¸¥é‡"
        elif crack_percentage > 5:
            severity += " - ä¸­ç­‰"
        else:
            severity += " - è½»å¾®"
    else:
        severity = "æœªæ£€æµ‹åˆ°æ˜æ˜¾è£‚ç¼"

    report = f"""
    ### æ£€æµ‹ç»“æœ
    - **è£‚ç¼å æ¯”**: {crack_percentage:.2f}%
    - **ä¸¥é‡ç¨‹åº¦**: {severity}
    """

    return mask_vis, overlay_image, report
```

### Gradio ç•Œé¢

```python
def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""

    app = CrackDetectionApp()

    with gr.Blocks(title="é“è·¯è£‚ç¼æ£€æµ‹ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ›£ï¸ é“è·¯è£‚ç¼æ£€æµ‹ç³»ç»Ÿ")

        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥
            with gr.Column():
                input_image = gr.Image(type="pil", label="ä¸Šä¼ é“è·¯å›¾ç‰‡")
                show_overlay = gr.Checkbox(value=True, label="æ˜¾ç¤ºå åŠ å›¾")
                overlay_alpha = gr.Slider(0, 1, value=0.5, label="å åŠ é€æ˜åº¦")
                predict_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹")

            # å³ä¾§ï¼šè¾“å‡º
            with gr.Column():
                output_mask = gr.Image(label="æ£€æµ‹ç»“æœ")
                output_overlay = gr.Image(label="å åŠ å¯è§†åŒ–")
                output_report = gr.Markdown(label="æ£€æµ‹æŠ¥å‘Š")

        # ç»‘å®šäº‹ä»¶
        predict_btn.click(
            fn=app.process_image,
            inputs=[input_image, show_overlay, overlay_alpha],
            outputs=[output_mask, output_overlay, output_report]
        )

    return demo
```

---

## å¦‚ä½•ä¿®æ”¹å’Œæ‰©å±•

### 1. è°ƒæ•´è¾“å…¥å›¾ç‰‡å°ºå¯¸

**ä¿®æ”¹ä½ç½®**ï¼š`model.py`, `dataset.py`, `app.py`

```python
# ä» 224Ã—224 æ”¹ä¸º 448Ã—448
img_size = (448, 448)

# æ³¨æ„ï¼šéœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼
```

**å½±å“**ï¼š
- âœ… æ›´é«˜çš„æ£€æµ‹ç²¾åº¦
- âŒ æ›´æ…¢çš„æ¨ç†é€Ÿåº¦
- âŒ æ›´å¤§çš„æ˜¾å­˜å ç”¨

### 2. å¢åŠ æ•°æ®å¢å¼º

**ä¿®æ”¹ä½ç½®**ï¼š`model_train.py`

```python
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    # æ–°å¢æ•°æ®å¢å¼º
    transforms.RandomHorizontalFlip(p=0.5),     # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomVerticalFlip(p=0.5),       # éšæœºå‚ç›´ç¿»è½¬
    transforms.RandomRotation(15),              # éšæœºæ—‹è½¬Â±15åº¦
    transforms.ColorJitter(                     # é¢œè‰²æŠ–åŠ¨
        brightness=0.2,
        contrast=0.2,
        saturation=0.2
    ),
    transforms.ToTensor(),
    normalize
])
```

### 3. ä¿®æ”¹æ¨¡å‹ç»“æ„

**ç¤ºä¾‹ï¼šå¢åŠ æ¨¡å‹æ·±åº¦**

```python
# åœ¨ model.py ä¸­æ·»åŠ ä¸€å±‚
self.down5 = nn.Sequential(
    nn.MaxPool2d(2),
    DoubleConv(1024, 2048)
)

# ç›¸åº”åœ°æ·»åŠ ä¸Šé‡‡æ ·å±‚
self.up0 = nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2)
self.conv0 = DoubleConv(2048, 1024)
```

### 4. æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

**ä¿®æ”¹ä½ç½®**ï¼š`model_train.py`

```python
def calculate_precision_recall(pred, target):
    """è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡"""
    pred = pred.view(-1)
    target = target.view(-1)

    # åªå…³æ³¨è£‚ç¼ç±»åˆ«
    tp = ((pred == 1) & (target == 1)).sum().float()  # çœŸé˜³æ€§
    fp = ((pred == 1) & (target == 0)).sum().float()  # å‡é˜³æ€§
    fn = ((pred == 0) & (target == 1)).sum().float()  # å‡é˜´æ€§

    precision = tp / (tp + fp + 1e-8)  # ç²¾ç¡®ç‡
    recall = tp / (tp + fn + 1e-8)     # å¬å›ç‡

    return precision.item(), recall.item()
```

### 5. ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ–‡ä»¶

**ä¿®æ”¹ä½ç½®**ï¼š`app.py`

```python
def process_image(self, input_image, ...):
    # ... åŸæœ‰ä»£ç  ...

    # æ–°å¢ï¼šä¿å­˜ç»“æœ
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜mask
    Image.fromarray(mask_vis).save(f"results/mask_{timestamp}.png")

    # ä¿å­˜å åŠ å›¾
    Image.fromarray(overlay_image).save(f"results/overlay_{timestamp}.png")

    # ä¿å­˜æŠ¥å‘Š
    with open(f"results/report_{timestamp}.txt", "w") as f:
        f.write(report)

    return mask_vis, overlay_image, report
```

### 6. æ·»åŠ å¤šç±»åˆ«åˆ†å‰²

**ç¤ºä¾‹ï¼šæ£€æµ‹å¤šç§é“è·¯ç¼ºé™·**

```python
# ä¿®æ”¹æ¨¡å‹è¾“å‡ºç±»åˆ«æ•°
model = UNet(n_channels=3, n_classes=4)  # èƒŒæ™¯ã€è£‚ç¼ã€å‘æ´¼ã€ç ´æŸ

# ä¿®æ”¹é¢œè‰²æ˜ å°„
color_map = {
    0: [0, 0, 0],       # èƒŒæ™¯ - é»‘è‰²
    1: [255, 0, 0],     # è£‚ç¼ - çº¢è‰²
    2: [0, 255, 0],     # å‘æ´¼ - ç»¿è‰²
    3: [0, 0, 255],     # ç ´æŸ - è“è‰²
}
```

---

## å¸¸è§ç¼–ç¨‹ä»»åŠ¡

### ä»»åŠ¡ 1: æ‰“å°æ¨¡å‹ç»“æ„

```python
from model import UNet

model = UNet(n_channels=3, n_classes=2)
print(model)

# æˆ–è€…ä½¿ç”¨ torchsummary
from torchsummary import summary
summary(model, input_size=(3, 224, 224))
```

### ä»»åŠ¡ 2: å¯è§†åŒ–è®­ç»ƒæ›²çº¿

```python
import matplotlib.pyplot as plt
import pandas as pd

# è¯»å–è®­ç»ƒå†å²
train_process = pd.read_csv('training_history.csv')

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_process.epoch, train_process.train_loss, label='Train')
plt.plot(train_process.epoch, train_process.val_loss, label='Val')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(1, 2, 2)
plt.plot(train_process.epoch, train_process.train_iou, label='Train')
plt.plot(train_process.epoch, train_process.val_iou, label='Val')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('IoU')

plt.show()
```

### ä»»åŠ¡ 3: è°ƒè¯•æ•°æ®åŠ è½½

```python
from dataset import CrackDataset
import matplotlib.pyplot as plt

dataset = CrackDataset('data/train/images', 'data/train/masks')

# æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬
image, mask = dataset[0]

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.imshow(image.permute(1, 2, 0))
plt.title('Image')

plt.subplot(1, 2, 2)
plt.imshow(mask.squeeze(), cmap='gray')
plt.title('Mask')

plt.show()
```

---

## è¿›é˜¶å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»

1. **U-Net åŸè®ºæ–‡**
   - æ ‡é¢˜: "U-Net: Convolutional Networks for Biomedical Image Segmentation"
   - é“¾æ¥: https://arxiv.org/abs/1505.04597

2. **PyTorch å®˜æ–¹æ•™ç¨‹**
   - å›¾åƒåˆ†å‰²æ•™ç¨‹: https://pytorch.org/tutorials/

3. **ç›¸å…³é¡¹ç›®**
   - Awesome Semantic Segmentation: https://github.com/mrgloom/awesome-semantic-segmentation

### å®è·µå»ºè®®

1. **ä»å°å®éªŒå¼€å§‹**ï¼šå…ˆç”¨å°‘é‡æ•°æ®ï¼ˆ50å¼ ï¼‰æµ‹è¯•ä»£ç 
2. **é€æ­¥å¢åŠ å¤æ‚åº¦**ï¼šå…ˆè·‘é€šåŸºç¡€ç‰ˆæœ¬ï¼Œå†æ·»åŠ åŠŸèƒ½
3. **å¤šåšå¯è§†åŒ–**ï¼šå¯è§†åŒ–ä¸­é—´ç»“æœæœ‰åŠ©äºç†è§£
4. **è®°å½•å®éªŒ**ï¼šè®°å½•æ¯æ¬¡ä¿®æ”¹çš„å‚æ•°å’Œç»“æœ

---

**æ­å–œå®Œæˆä»£ç å­¦ä¹ ï¼** ç°åœ¨ä½ åº”è¯¥å¯¹æ•´ä¸ªé¡¹ç›®æœ‰äº†å…¨é¢çš„ç†è§£ã€‚å¼€å§‹åŠ¨æ‰‹å®è·µå§ï¼
