# ğŸ“ æ¨¡å‹è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—å°†å¸¦ä½ ä¸€æ­¥æ­¥å®Œæˆ U-Net æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ“š ç›®å½•

1. [è®­ç»ƒå‰å‡†å¤‡](#è®­ç»ƒå‰å‡†å¤‡)
2. [ç†è§£è®­ç»ƒè¿‡ç¨‹](#ç†è§£è®­ç»ƒè¿‡ç¨‹)
3. [å¼€å§‹è®­ç»ƒ](#å¼€å§‹è®­ç»ƒ)
4. [ç›‘æ§è®­ç»ƒè¿›åº¦](#ç›‘æ§è®­ç»ƒè¿›åº¦)
5. [å‚æ•°è°ƒæ•´](#å‚æ•°è°ƒæ•´)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## è®­ç»ƒå‰å‡†å¤‡

### 1. æ£€æŸ¥ç¯å¢ƒ

ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆéœ€è¦ 3.7+ï¼‰
python --version

# æ£€æŸ¥ PyTorch æ˜¯å¦å®‰è£…
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"

# æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ï¼ˆGPUåŠ é€Ÿï¼‰
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
```

### 2. æ£€æŸ¥æ•°æ®é›†

```bash
# è¿è¡Œæ•°æ®é›†æ£€æŸ¥è„šæœ¬
python -c "
import os
for split in ['train', 'val']:
    img_dir = f'data/{split}/images'
    mask_dir = f'data/{split}/masks'
    if os.path.exists(img_dir):
        n_imgs = len(os.listdir(img_dir))
        n_masks = len(os.listdir(mask_dir))
        print(f'{split}: {n_imgs} å›¾ç‰‡, {n_masks} æ ‡æ³¨')
    else:
        print(f'âŒ {split} æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
train: 500 å›¾ç‰‡, 500 æ ‡æ³¨
val: 100 å›¾ç‰‡, 100 æ ‡æ³¨
```

### 3. äº†è§£ç¡¬ä»¶é…ç½®

è®­ç»ƒæ—¶é—´å’Œé…ç½®çš„å…³ç³»ï¼š

| ç¡¬ä»¶é…ç½® | è®­ç»ƒé€Ÿåº¦ | æ¨èæ‰¹æ¬¡å¤§å° | é¢„è®¡æ—¶é—´ï¼ˆ40è½®ï¼‰|
|---------|---------|------------|----------------|
| **CPU only** | æ…¢ | 4-8 | æ•°å°æ—¶ |
| **1ä¸ª GPU (4GB)** | ä¸­ç­‰ | 16-32 | 1-2å°æ—¶ |
| **1ä¸ª GPU (8GB+)** | å¿« | 64-128 | 30-60åˆ†é’Ÿ |
| **å¤šä¸ª GPU** | å¾ˆå¿« | è‡ªåŠ¨è°ƒæ•´ | 20-30åˆ†é’Ÿ |

---

## ç†è§£è®­ç»ƒè¿‡ç¨‹

### ä»€ä¹ˆæ˜¯è®­ç»ƒï¼Ÿ

è®­ç»ƒå°±æ˜¯è®©æ¨¡å‹"å­¦ä¹ "å¦‚ä½•è¯†åˆ«è£‚ç¼çš„è¿‡ç¨‹ã€‚ç±»æ¯”ï¼š

- ğŸ“– **è®­ç»ƒé›†** = ç»ƒä¹ é¢˜ï¼ˆæ¨¡å‹ä»ä¸­å­¦ä¹ ï¼‰
- ğŸ“ **éªŒè¯é›†** = æ¨¡æ‹Ÿè€ƒè¯•ï¼ˆæ£€éªŒå­¦ä¹ æ•ˆæœï¼‰
- ğŸ¯ **ç›®æ ‡** = è®©æ¨¡å‹èƒ½å‡†ç¡®è¯†åˆ«è£‚ç¼

### å…³é”®æ¦‚å¿µ

#### 1. Epochï¼ˆè½®æ¬¡ï¼‰

**å®šä¹‰**ï¼šæ¨¡å‹çœ‹å®Œæ•´ä¸ªè®­ç»ƒé›†ä¸€éï¼Œç§°ä¸º 1 ä¸ª epoch

**ç¤ºä¾‹**ï¼š
- è®­ç»ƒé›†æœ‰ 500 å¼ å›¾
- è®­ç»ƒ 40 ä¸ª epoch = æ¨¡å‹ä¼šçœ‹è¿™ 500 å¼ å›¾ 40 é
- æ¯æ¬¡çœ‹éƒ½ä¼šè°ƒæ•´å‚æ•°ï¼Œé€æ¸æé«˜å‡†ç¡®åº¦

#### 2. Batch Sizeï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰

**å®šä¹‰**ï¼šæ¯æ¬¡åŒæ—¶å¤„ç†å¤šå°‘å¼ å›¾ç‰‡

**ç¤ºä¾‹**ï¼š
- Batch size = 16
- 500 å¼ å›¾ Ã· 16 = 32 ä¸ª batch
- æ¨¡å‹æ¯æ¬¡å¤„ç† 16 å¼ å›¾ï¼Œæ›´æ–°ä¸€æ¬¡å‚æ•°

**å¦‚ä½•é€‰æ‹©**ï¼š
- GPU æ˜¾å­˜è¶Šå¤§ï¼Œbatch size å¯ä»¥è¶Šå¤§
- å¤ªå°ï¼šè®­ç»ƒæ…¢ï¼Œä¸ç¨³å®š
- å¤ªå¤§ï¼šæ˜¾å­˜ä¸è¶³ï¼Œç¨‹åºå´©æºƒ
- **æ¨èå€¼**ï¼š16-64

#### 3. Learning Rateï¼ˆå­¦ä¹ ç‡ï¼‰

**å®šä¹‰**ï¼šæ¨¡å‹æ¯æ¬¡è°ƒæ•´å‚æ•°çš„æ­¥é•¿

**ç±»æ¯”**ï¼š
- å­¦ä¹ ç‡å¤ªå¤§ = èµ°è·¯æ­¥å­å¤ªå¤§ï¼Œå®¹æ˜“è·¨è¿‡æœ€ä¼˜ç‚¹
- å­¦ä¹ ç‡å¤ªå° = æ­¥å­å¤ªå°ï¼Œè®­ç»ƒå¾ˆæ…¢
- **æ¨èå€¼**ï¼š0.001ï¼ˆé»˜è®¤ï¼‰

#### 4. Lossï¼ˆæŸå¤±å‡½æ•°ï¼‰

**å®šä¹‰**ï¼šæ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡æ³¨çš„å·®è·

- Loss è¶Šå° = é¢„æµ‹è¶Šå‡†ç¡®
- è®­ç»ƒç›®æ ‡ = é™ä½ Loss

**æœ¬é¡¹ç›®ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°**ï¼š
- **Dice Loss (60%)**: ä¸“é—¨é’ˆå¯¹åˆ†å‰²ä»»åŠ¡ï¼Œå¯¹å°ç›®æ ‡ï¼ˆè£‚ç¼ï¼‰æ›´æ•æ„Ÿ
- **Cross Entropy Loss (40%)**: æä¾›ç¨³å®šçš„æ¢¯åº¦ï¼Œå¸®åŠ©æ”¶æ•›

è¿™ç§ç»„åˆç‰¹åˆ«é€‚åˆé“è·¯è£‚ç¼æ£€æµ‹ï¼Œå› ä¸ºè£‚ç¼é€šå¸¸åªå å›¾åƒå¾ˆå°éƒ¨åˆ†ã€‚

**æŸ¥çœ‹æ–¹å¼**ï¼š
```
ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°: Dice Loss (60%) + Cross Entropy Loss (40%)

Epoch 0/39
----------
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 32/32
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 8/8

0 Train Loss: 0.4521 Train IoU: 0.6234  â† Loss é™ä½ï¼ŒIoU ä¸Šå‡æ˜¯å¥½ç°è±¡
0 Val Loss: 0.4012 Val IoU: 0.6789
å½“å‰å­¦ä¹ ç‡: 0.001000  â† å­¦ä¹ ç‡ä¼šé€æ¸é™ä½
```

#### 5. IoUï¼ˆIntersection over Unionï¼‰

**å®šä¹‰**ï¼šè¡¡é‡åˆ†å‰²å‡†ç¡®åº¦çš„æŒ‡æ ‡

**è®¡ç®—æ–¹å¼**ï¼š
```
IoU = é¢„æµ‹æ­£ç¡®çš„åƒç´  / (é¢„æµ‹çš„åƒç´  + çœŸå®çš„åƒç´  - é¢„æµ‹æ­£ç¡®çš„åƒç´ )
```

**å–å€¼èŒƒå›´**ï¼š
- 0 = å®Œå…¨é”™è¯¯
- 1 = å®Œå…¨æ­£ç¡®
- **å¥½æ¨¡å‹**ï¼šIoU > 0.7

---

## å¼€å§‹è®­ç»ƒ

### æ–¹æ³• 1: ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆæ¨èåˆå­¦è€…ï¼‰

ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

```bash
python model_train.py
```

### æ–¹æ³• 2: è‡ªå®šä¹‰å‚æ•°

ä¿®æ”¹ `model_train.py` ä¸­çš„å‚æ•°ï¼š

```python
# åœ¨æ–‡ä»¶æœ«å°¾æ‰¾åˆ°è¿™éƒ¨åˆ†ä»£ç 
if __name__ == "__main__":
    # GPU è®¾ç½®
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ 0 å· GPU

    # è®­ç»ƒå‚æ•°
    num_epochs = 40        # è®­ç»ƒè½®æ•°ï¼ˆå¯æ”¹ä¸º 20, 60 ç­‰ï¼‰
    batch_size = 91        # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
    learning_rate = 0.001  # å­¦ä¹ ç‡

    # åˆå§‹åŒ–æ¨¡å‹
    model = UNet(n_channels=3, n_classes=2)

    # å¼€å§‹è®­ç»ƒ
    train_process = train_model_process(
        model,
        train_dataloader,
        val_dataloader,
        num_epochs  # è¿™é‡Œä½¿ç”¨ä¸Šé¢å®šä¹‰çš„è½®æ•°
    )
```

### è®­ç»ƒæµç¨‹è¯¦è§£

#### ç¬¬ 1 æ­¥ï¼šæ•°æ®åŠ è½½

```
åŠ è½½æ•°æ®é›†...
âœ“ åŠ è½½æ•°æ®é›†: 500 å¼ å›¾ç‰‡
  å›¾ç‰‡è·¯å¾„: data/train/images
  æ©è†œè·¯å¾„: data/train/masks
âœ“ åŠ è½½æ•°æ®é›†: 100 å¼ å›¾ç‰‡
  å›¾ç‰‡è·¯å¾„: data/val/images
  æ©è†œè·¯å¾„: data/val/masks
Training set data size: 500, Validating set data size: 100
ä½¿ç”¨ 1 ä¸ªGPU, æ€»batch size: 91
```

#### ç¬¬ 2 æ­¥ï¼šæ¨¡å‹åˆå§‹åŒ–

```
æ£€æµ‹åˆ° 1 ä¸ªGPU
  GPU 0: NVIDIA GeForce RTX 3080
å¼€å§‹è®­ç»ƒ...
```

#### ç¬¬ 3 æ­¥ï¼šè®­ç»ƒå¾ªç¯

æ¯ä¸ª epoch ä¼šæ˜¾ç¤ºï¼š

```
Epoch 0/39
----------
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:15<00:00,  2.58s/it]

Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.12s/it]

0 Train Loss: 0.4521 Train IoU: 0.6234
0 Val Loss: 0.4012 Val IoU: 0.6789

--------------------------------------------------

è®­ç»ƒå’ŒéªŒè¯è€—è´¹çš„æ—¶é—´1m23s
```

**è§£è¯»**ï¼š
- **Train Loss ä¸‹é™** âœ… = æ¨¡å‹åœ¨å­¦ä¹ 
- **Val IoU ä¸Šå‡** âœ… = éªŒè¯é›†è¡¨ç°æå‡
- **Val Loss < Train Loss** âœ… = æ¨¡å‹æ³›åŒ–è‰¯å¥½

#### ç¬¬ 4 æ­¥ï¼šä¿å­˜æœ€ä½³æ¨¡å‹

```
æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: best_model.pth, æœ€ä½³IoU: 0.8234
```

åªæœ‰å½“éªŒè¯é›† IoU åˆ›æ–°é«˜æ—¶ï¼Œæ‰ä¼šä¿å­˜æ¨¡å‹ã€‚

#### ç¬¬ 5 æ­¥ï¼šç”Ÿæˆè®­ç»ƒæ›²çº¿

```
è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° training_curves.png
```

æ‰“å¼€å›¾ç‰‡å¯ä»¥çœ‹åˆ° Loss å’Œ IoU çš„å˜åŒ–è¶‹åŠ¿ã€‚

---

## ç›‘æ§è®­ç»ƒè¿›åº¦

### 1. å®æ—¶è§‚å¯Ÿè¾“å‡º

**å¥½çš„è®­ç»ƒè¿¹è±¡** âœ…ï¼š
- Train Loss æŒç»­ä¸‹é™
- Val Loss æŒç»­ä¸‹é™
- Train IoU æŒç»­ä¸Šå‡
- Val IoU æŒç»­ä¸Šå‡
- Val IoU æ¥è¿‘æˆ–ç•¥ä½äº Train IoU

**è­¦å‘Šè¿¹è±¡** âš ï¸ï¼š
- Loss ä¸ä¸‹é™æˆ–ä¸Šå‡
- Val Loss è¿œé«˜äº Train Lossï¼ˆè¿‡æ‹Ÿåˆï¼‰
- IoU é•¿æ—¶é—´ä¸æå‡
- Loss å‡ºç° NaN

### 2. æŸ¥çœ‹è®­ç»ƒæ›²çº¿

è®­ç»ƒå®Œæˆåï¼Œæ‰“å¼€ `training_curves.png`ï¼š

**ç†æƒ³æ›²çº¿**ï¼š
```
Loss Curve:
  Train Loss â†˜ é€æ¸ä¸‹é™
  Val Loss   â†˜ è·Ÿéšä¸‹é™

IoU Curve:
  Train IoU â†— é€æ¸ä¸Šå‡
  Val IoU   â†— è·Ÿéšä¸Šå‡
```

**è¿‡æ‹Ÿåˆæ›²çº¿**ï¼š
```
Loss Curve:
  Train Loss â†˜ æŒç»­ä¸‹é™
  Val Loss   â†— å¼€å§‹ä¸Šå‡ â† è­¦å‘Šï¼

è§£å†³æ–¹æ³•ï¼š
- å¢åŠ è®­ç»ƒæ•°æ®
- ä½¿ç”¨æ•°æ®å¢å¼º
- å‡å°‘è®­ç»ƒè½®æ•°
- æ·»åŠ æ­£åˆ™åŒ–
```


## å‚æ•°è°ƒæ•´

### ä½•æ—¶éœ€è¦è°ƒæ•´å‚æ•°ï¼Ÿ

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| è®­ç»ƒå¤ªæ…¢ | Batch size å¤ªå° | å¢å¤§ batch size |
| æ˜¾å­˜ä¸è¶³ | Batch size å¤ªå¤§ | å‡å° batch size |
| Loss ä¸ä¸‹é™ | å­¦ä¹ ç‡ä¸å½“ | è°ƒæ•´å­¦ä¹ ç‡ |
| è¿‡æ‹Ÿåˆ | è®­ç»ƒè½®æ•°å¤ªå¤š | å‡å°‘ epoch |
| æ¬ æ‹Ÿåˆ | è®­ç»ƒä¸å……åˆ† | å¢åŠ  epoch |

### å¸¸ç”¨å‚æ•°ç»„åˆ

#### 1. å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯ä»£ç èƒ½å¦è¿è¡Œï¼‰

```python
num_epochs = 5          # åªè®­ç»ƒ5è½®
batch_size = 16         # è¾ƒå°çš„batch
learning_rate = 0.001
```

#### 2. æ ‡å‡†è®­ç»ƒï¼ˆæ¨èï¼‰

```python
num_epochs = 40         # å……åˆ†è®­ç»ƒ
batch_size = 32         # å¹³è¡¡æ˜¾å­˜å’Œé€Ÿåº¦
learning_rate = 0.001
```

#### 3. ç²¾ç»†è®­ç»ƒï¼ˆè¿½æ±‚é«˜ç²¾åº¦ï¼‰

```python
num_epochs = 100        # æ›´å¤šè½®æ¬¡
batch_size = 64         # æ›´å¤§batchï¼ˆéœ€è¦è¶³å¤Ÿæ˜¾å­˜ï¼‰
learning_rate = 0.0001  # æ›´å°å­¦ä¹ ç‡
```

#### 4. GPU æ˜¾å­˜ä¸è¶³

```python
num_epochs = 40
batch_size = 8          # å‡å°batch
learning_rate = 0.001
num_workers = 2         # å‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹
```

### å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥

**æœ¬é¡¹ç›®å·²å®ç°ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼š

```python
# å·²åœ¨ model_train.py ä¸­é…ç½®
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ä½™å¼¦é€€ç«ç­–ç•¥ï¼šå­¦ä¹ ç‡ä» 0.001 å¹³æ»‘é™è‡³ 1e-6
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=num_epochs,  # æ€»è½®æ•°
    eta_min=1e-6       # æœ€å°å­¦ä¹ ç‡
)
```

**ä¼˜åŠ¿**ï¼š
- å­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿å¹³æ»‘ä¸‹é™
- è®­ç»ƒåˆæœŸï¼šå¤§å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›
- è®­ç»ƒåæœŸï¼šå°å­¦ä¹ ç‡ç²¾ç»†è°ƒä¼˜
- é¿å…çªç„¶çš„å­¦ä¹ ç‡å˜åŒ–

**ç›‘æ§å­¦ä¹ ç‡**ï¼š
æ¯ä¸ªepochç»“æŸä¼šæ˜¾ç¤ºå½“å‰å­¦ä¹ ç‡ï¼š
```
0 Train Loss: 0.4521 Train IoU: 0.6234
0 Val Loss: 0.4012 Val IoU: 0.6789
å½“å‰å­¦ä¹ ç‡: 0.001000  â† ç¬¬1è½®

...

39 Train Loss: 0.1234 Train IoU: 0.8567
39 Val Loss: 0.1456 Val IoU: 0.8234
å½“å‰å­¦ä¹ ç‡: 0.000001  â† ç¬¬40è½®å·²é™è‡³æœ€å°å€¼
```

---

## å¸¸è§é—®é¢˜

### Q1: CUDA out of memoryï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate XX.XX MiB
```

**è§£å†³æ–¹æ³•**ï¼š

1. **å‡å° batch size**
```python
batch_size = 16  # ä» 64 æ”¹ä¸º 16
```

2. **å‡å°‘ num_workers**
```python
num_workers = 2  # ä» 12 æ”¹ä¸º 2
```

3. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**ï¼ˆæ¨¡æ‹Ÿå¤§ batchï¼‰
```python
accumulation_steps = 4
for i, (images, masks) in enumerate(train_loader):
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### Q2: Loss å˜æˆ NaN

**å¯èƒ½åŸå› **ï¼š
- å­¦ä¹ ç‡å¤ªå¤§
- æ•°æ®å¼‚å¸¸ï¼ˆå­˜åœ¨æ— ç©·å¤§æˆ–NaNï¼‰

**è§£å†³æ–¹æ³•**ï¼š

```python
# 1. é™ä½å­¦ä¹ ç‡
learning_rate = 0.0001  # ä» 0.001 æ”¹ä¸º 0.0001

# 2. æ·»åŠ æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. æ£€æŸ¥æ•°æ®
print(f"å›¾ç‰‡èŒƒå›´: [{image.min()}, {image.max()}]")
print(f"æ ‡æ³¨èŒƒå›´: [{mask.min()}, {mask.max()}]")
```

### Q3: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**ä¼˜åŒ–æ–¹æ³•**ï¼š

1. **å¢å¤§ batch size**ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
```python
batch_size = 64
```

2. **ä½¿ç”¨æ›´å¤š workers**
```python
num_workers = 8  # åˆ©ç”¨å¤šæ ¸CPU
```

3. **å¯ç”¨ pin_memory**
```python
train_loader = DataLoader(..., pin_memory=True)
```

4. **ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**ï¼ˆéœ€è¦ GPUï¼‰
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(images)
    loss = criterion(output, masks)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### Q4: éªŒè¯é›† IoU ä¸æå‡

**å¯èƒ½åŸå› **ï¼š
- è®­ç»ƒæ•°æ®ä¸è¶³
- æ¨¡å‹è¿‡æ‹Ÿåˆ
- éªŒè¯é›†ä¸è®­ç»ƒé›†åˆ†å¸ƒä¸åŒ

**è§£å†³æ–¹æ³•**ï¼š

1. **æ•°æ®å¢å¼º**
```python
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),      # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomVerticalFlip(),        # éšæœºå‚ç›´ç¿»è½¬
    transforms.ColorJitter(0.2, 0.2, 0.2), # é¢œè‰²æŠ–åŠ¨
    transforms.ToTensor(),
    normalize
])
```

2. **æ—©åœæ³•**ï¼ˆEarly Stoppingï¼‰
```python
patience = 10  # 10è½®ä¸æå‡å°±åœæ­¢
best_val_iou = 0
patience_counter = 0

for epoch in range(num_epochs):
    # ... è®­ç»ƒä»£ç  ...

    if val_iou > best_val_iou:
        best_val_iou = val_iou
        patience_counter = 0
        # ä¿å­˜æ¨¡å‹
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print("æ—©åœï¼éªŒè¯é›†IoUä¸å†æå‡")
        break
```

3. **ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**
```python
# åŠ è½½ ImageNet é¢„è®­ç»ƒæƒé‡
import torchvision.models as models

# ä½¿ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨
encoder = models.resnet18(pretrained=True)
```

### Q5: å¦‚ä½•ä»ä¸­æ–­çš„è®­ç»ƒç»§ç»­ï¼Ÿ

**ä¿å­˜æ£€æŸ¥ç‚¹**ï¼š

```python
# ä¿å­˜
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'best_iou': best_iou,
}
torch.save(checkpoint, 'checkpoint.pth')

# æ¢å¤
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
best_iou = checkpoint['best_iou']

# ä» start_epoch ç»§ç»­è®­ç»ƒ
for epoch in range(start_epoch, num_epochs):
    # ...
```

---

## è®­ç»ƒå®Œæˆå

### 1. è¯„ä¼°æ¨¡å‹

```bash
python model_test.py
```

### 2. æŸ¥çœ‹ç»“æœ

æ£€æŸ¥ `test_results/` æ–‡ä»¶å¤¹ä¸­çš„å¯è§†åŒ–ç»“æœã€‚

### 3. éƒ¨ç½²åº”ç”¨

```bash
python app.py
```

---

## ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹ [ä½¿ç”¨éƒ¨ç½²æŒ‡å—](./03_ä½¿ç”¨éƒ¨ç½²æŒ‡å—.md) å­¦ä¹ å¦‚ä½•ä½¿ç”¨å’Œéƒ¨ç½²æ¨¡å‹ï¼

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** é‡åˆ°é—®é¢˜è¯·æŸ¥çœ‹ Issues æˆ–æäº¤æ–°é—®é¢˜ã€‚
